{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gen AI Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple GenAI App using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_KEY'] = os.getenv('LANGCHAIN_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "# Scrape the data from the website\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "# loader = WebBaseLoader(\"https://docs.smith.langchain.com/tutorial/Administrators/manage_speed\")\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/evaluation/how_to_guides\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x15fb0457880>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nEvaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedbackHow to run an evaluationHow to manage datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationHow-to GuidesOn this pageEvaluation how-to guides\\nThese guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.\\nThey are goal-oriented and concrete, and are meant to help you complete a specific task.\\nFor conceptual explanations see the Conceptual guide.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.\\nKey features\\u200b\\n\\nCreate a dataset with the SDK or from the UI\\nRun offline evaluations with the SDK or from the UI\\nRun online evaluations with LLM-as-judge and custom code evaluators\\nAnalyze evaluation results in the UI\\nLog user feedback from your app\\nLog expert feedback with annotation queues\\n\\nOffline evaluation\\u200b\\nEvaluate and improve your application before deploying it.\\nRun an evaluation\\u200b\\n\\nDefine a target function to evaluate\\nRun an evaluation with the SDK\\nRun an evaluation asynchronously\\nRun an evaluation comparing two experiments\\nEvaluate a langchain runnable\\nEvaluate a langgraph graph\\nEvaluate an existing experiment (Python only)\\nRun an evaluation from the UI\\nRun an evaluation via the REST API\\nRun an evaluation with large file inputs\\n\\nDefine an evaluator\\u200b\\n\\nDefine a custom evaluator\\nDefine an LLM-as-a-judge evaluator\\nDefine a pairwise evaluator\\nDefine a summary evaluator\\nUse prebuilt evaluators\\nEvaluate an application's intermediate steps\\nReturn multiple metrics in one evaluator\\nReturn categorical vs numerical metrics\\n\\nConfigure the evaluation data\\u200b\\n\\nEvaluate on a split / filtered view of a dataset\\nEvaluate on a specific dataset version\\n\\nConfigure an evaluation job\\u200b\\n\\nEvaluate with repetitions\\nHandle model rate limits\\nPrint detailed logs (Python only)\\nRun an evaluation locally (beta, Python only)\\n\\nTesting integrations\\u200b\\nRun evals using your favorite testing tools.\\n\\nRun evals with pytest (beta)\\nRun evals with Vitest/Jest (beta)\\n\\nOnline evaluation\\u200b\\nEvaluate and monitor your system's live performance on production data.\\n\\nSet up an LLM-as-judge online evaluator\\nSet up a custom code online evaluator\\nCreate a few-shot evaluator\\n\\nAutomatic evaluation\\u200b\\nSet up evaluators that automatically run for all experiments against a dataset.\\n\\nSet up an auto-evaluator\\nCreate a few-shot evaluator\\n\\nAnalyzing experiment results\\u200b\\nUse the UI & API to understand your experiment results.\\n\\nAnalyze a single experiment\\nCompare experiments with the comparison view\\nFilter experiments\\nView pairwise experiments\\nFetch experiment results in the SDK\\nUpload experiments run outside of LangSmith with the REST API\\nDownload experiment results as a CSV\\nAudit and correct evaluator scores\\nRenaming an experiment\\n\\nDataset management\\u200b\\nManage datasets in LangSmith used by your evaluations.\\n\\nCreate a dataset from the UI\\nExport a dataset from the UI\\nCreate a dataset split from the UI\\nFilter examples from the UI\\nCreate a dataset with the SDK\\nFetch a dataset with the SDK\\nUpdate a dataset with the SDK\\nVersion a dataset\\nShare/unshare a dataset publicly\\nExport filtered traces from an experiment to a dataset\\n\\nAnnotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.\\n\\nUse annotation queues\\nLog user feedback\\nSet up a new feedback criteria\\nAnnotate traces inline in the UI\\nAudit and correct evaluator scores\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousTest a ReAct agent with Pytest/Vitest and LangSmithNextAnalyze a single experimentKey featuresOffline evaluationRun an evaluationDefine an evaluatorConfigure the evaluation dataConfigure an evaluation jobTesting integrationsOnline evaluationAutomatic evaluationAnalyzing experiment resultsDataset managementAnnotation queues and human feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides',\n",
       " 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith',\n",
       " 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nEvaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedbackHow to run an evaluationHow to manage datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationHow-to GuidesOn this pageEvaluation how-to guides\\nThese guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.\\nThey are goal-oriented and concrete, and are meant to help you complete a specific task.\\nFor conceptual explanations see the Conceptual guide.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.\\nKey features\\u200b\\n\\nCreate a dataset with the SDK or from the UI\\nRun offline evaluations with the SDK or from the UI\\nRun online evaluations with LLM-as-judge and custom code evaluators\\nAnalyze evaluation results in the UI\\nLog user feedback from your app\\nLog expert feedback with annotation queues\\n\\nOffline evaluation\\u200b\\nEvaluate and improve your application before deploying it.\\nRun an evaluation\\u200b\\n\\nDefine a target function to evaluate\\nRun an evaluation with the SDK\\nRun an evaluation asynchronously\\nRun an evaluation comparing two experiments\\nEvaluate a langchain runnable\\nEvaluate a langgraph graph\\nEvaluate an existing experiment (Python only)\\nRun an evaluation from the UI\\nRun an evaluation via the REST API\\nRun an evaluation with large file inputs\\n\\nDefine an evaluator\\u200b\\n\\nDefine a custom evaluator\\nDefine an LLM-as-a-judge evaluator\\nDefine a pairwise evaluator\\nDefine a summary evaluator\\nUse prebuilt evaluators\\nEvaluate an application's intermediate steps\\nReturn multiple metrics in one evaluator\\nReturn categorical vs numerical metrics\\n\\nConfigure the evaluation data\\u200b\\n\\nEvaluate on a split / filtered view of a dataset\\nEvaluate on a specific dataset version\\n\\nConfigure an evaluation job\\u200b\\n\\nEvaluate with repetitions\\nHandle model rate limits\\nPrint detailed logs (Python only)\\nRun an evaluation locally (beta, Python only)\\n\\nTesting integrations\\u200b\\nRun evals using your favorite testing tools.\\n\\nRun evals with pytest (beta)\\nRun evals with Vitest/Jest (beta)\\n\\nOnline evaluation\\u200b\\nEvaluate and monitor your system's live performance on production data.\\n\\nSet up an LLM-as-judge online evaluator\\nSet up a custom code online evaluator\\nCreate a few-shot evaluator\\n\\nAutomatic evaluation\\u200b\\nSet up evaluators that automatically run for all experiments against a dataset.\\n\\nSet up an auto-evaluator\\nCreate a few-shot evaluator\\n\\nAnalyzing experiment results\\u200b\\nUse the UI & API to understand your experiment results.\\n\\nAnalyze a single experiment\\nCompare experiments with the comparison view\\nFilter experiments\\nView pairwise experiments\\nFetch experiment results in the SDK\\nUpload experiments run outside of LangSmith with the REST API\\nDownload experiment results as a CSV\\nAudit and correct evaluator scores\\nRenaming an experiment\\n\\nDataset management\\u200b\\nManage datasets in LangSmith used by your evaluations.\\n\\nCreate a dataset from the UI\\nExport a dataset from the UI\\nCreate a dataset split from the UI\\nFilter examples from the UI\\nCreate a dataset with the SDK\\nFetch a dataset with the SDK\\nUpdate a dataset with the SDK\\nVersion a dataset\\nShare/unshare a dataset publicly\\nExport filtered traces from an experiment to a dataset\\n\\nAnnotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.\\n\\nUse annotation queues\\nLog user feedback\\nSet up a new feedback criteria\\nAnnotate traces inline in the UI\\nAudit and correct evaluator scores\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousTest a ReAct agent with Pytest/Vitest and LangSmithNextAnalyze a single experimentKey featuresOffline evaluationRun an evaluationDefine an evaluatorConfigure the evaluation dataConfigure an evaluation jobTesting integrationsOnline evaluationAutomatic evaluationAnalyzing experiment resultsDataset managementAnnotation queues and human feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data ---> Docs ---> Divide text to chunks ---> Convert text to vectors using vector embedding ---> Store in Vectorstore db\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content=\"Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedbackHow to run an evaluationHow to manage datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationHow-to GuidesOn this pageEvaluation how-to guides'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.\\nThey are goal-oriented and concrete, and are meant to help you complete a specific task.\\nFor conceptual explanations see the Conceptual guide.\\nFor end-to-end walkthroughs see Tutorials.\\nFor comprehensive descriptions of every class and function see the API reference.\\nKey features\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Create a dataset with the SDK or from the UI\\nRun offline evaluations with the SDK or from the UI\\nRun online evaluations with LLM-as-judge and custom code evaluators\\nAnalyze evaluation results in the UI\\nLog user feedback from your app\\nLog expert feedback with annotation queues\\n\\nOffline evaluation\\u200b\\nEvaluate and improve your application before deploying it.\\nRun an evaluation\\u200b\\n\\nDefine a target function to evaluate\\nRun an evaluation with the SDK\\nRun an evaluation asynchronously\\nRun an evaluation comparing two experiments\\nEvaluate a langchain runnable\\nEvaluate a langgraph graph\\nEvaluate an existing experiment (Python only)\\nRun an evaluation from the UI\\nRun an evaluation via the REST API\\nRun an evaluation with large file inputs\\n\\nDefine an evaluator\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content=\"Define an evaluator\\u200b\\n\\nDefine a custom evaluator\\nDefine an LLM-as-a-judge evaluator\\nDefine a pairwise evaluator\\nDefine a summary evaluator\\nUse prebuilt evaluators\\nEvaluate an application's intermediate steps\\nReturn multiple metrics in one evaluator\\nReturn categorical vs numerical metrics\\n\\nConfigure the evaluation data\\u200b\\n\\nEvaluate on a split / filtered view of a dataset\\nEvaluate on a specific dataset version\\n\\nConfigure an evaluation job\\u200b\\n\\nEvaluate with repetitions\\nHandle model rate limits\\nPrint detailed logs (Python only)\\nRun an evaluation locally (beta, Python only)\\n\\nTesting integrations\\u200b\\nRun evals using your favorite testing tools.\\n\\nRun evals with pytest (beta)\\nRun evals with Vitest/Jest (beta)\\n\\nOnline evaluation\\u200b\\nEvaluate and monitor your system's live performance on production data.\\n\\nSet up an LLM-as-judge online evaluator\\nSet up a custom code online evaluator\\nCreate a few-shot evaluator\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content=\"Online evaluation\\u200b\\nEvaluate and monitor your system's live performance on production data.\\n\\nSet up an LLM-as-judge online evaluator\\nSet up a custom code online evaluator\\nCreate a few-shot evaluator\\n\\nAutomatic evaluation\\u200b\\nSet up evaluators that automatically run for all experiments against a dataset.\\n\\nSet up an auto-evaluator\\nCreate a few-shot evaluator\\n\\nAnalyzing experiment results\\u200b\\nUse the UI & API to understand your experiment results.\\n\\nAnalyze a single experiment\\nCompare experiments with the comparison view\\nFilter experiments\\nView pairwise experiments\\nFetch experiment results in the SDK\\nUpload experiments run outside of LangSmith with the REST API\\nDownload experiment results as a CSV\\nAudit and correct evaluator scores\\nRenaming an experiment\\n\\nDataset management\\u200b\\nManage datasets in LangSmith used by your evaluations.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Dataset management\\u200b\\nManage datasets in LangSmith used by your evaluations.\\n\\nCreate a dataset from the UI\\nExport a dataset from the UI\\nCreate a dataset split from the UI\\nFilter examples from the UI\\nCreate a dataset with the SDK\\nFetch a dataset with the SDK\\nUpdate a dataset with the SDK\\nVersion a dataset\\nShare/unshare a dataset publicly\\nExport filtered traces from an experiment to a dataset\\n\\nAnnotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Annotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.\\n\\nUse annotation queues\\nLog user feedback\\nSet up a new feedback criteria\\nAnnotate traces inline in the UI\\nAudit and correct evaluator scores\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousTest a ReAct agent with Pytest/Vitest and LangSmithNextAnalyze a single experimentKey featuresOffline evaluationRun an evaluationDefine an evaluatorConfigure the evaluation dataConfigure an evaluation jobTesting integrationsOnline evaluationAutomatic evaluationAnalyzing experiment resultsDataset managementAnnotation queues and human feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x15fd1532170>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM model\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.\\n\\nUse annotation queues\\nLog user feedback\\nSet up a new feedback criteria\\nAnnotate traces inline in the UI\\nAudit and correct evaluator scores\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousTest a ReAct agent with Pytest/Vitest and LangSmithNextAnalyze a single experimentKey featuresOffline evaluationRun an evaluationDefine an evaluatorConfigure the evaluation dataConfigure an evaluation jobTesting integrationsOnline evaluationAutomatic evaluationAnalyzing experiment resultsDataset managementAnnotation queues and human feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query from vectorstore db\n",
    "uery = \" LLMs are non-deterministic by nature, meaning they can produce unexpected results\"\n",
    "query1 = \"Evaluate your application over production traffic\"\n",
    "\n",
    "results = vectorstoredb.similarity_search(query1)\n",
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following questions based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000016025DD84F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000016025D57DF0>, root_client=<openai.OpenAI object at 0x0000016025DD8040>, root_async_client=<openai.AsyncOpenAI object at 0x0000016025DD96F0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following questions based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To evaluate your application over production traffic, you could consider conducting a performance assessment by gathering and analyzing data on how your application performs under real-world conditions. This involves monitoring metrics such as response time, error rates, system load, and user satisfaction. Additionally, you might want to simulate different load scenarios to see how the application handles peak usage. Doing this helps identify any bottlenecks, inefficiencies, or areas that require optimization to ensure the application performs reliably and efficiently for users.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\": \"Evaluate your application over production traffic\",\n",
    "    \"context\": [Document(page_content=\"Evaluate your application over production traffic\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x15fd1532170>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrievers\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input ---> Retriever ---> Vectorstoredb\n",
    "\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000015FD1532170>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following questions based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000016025DD84F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000016025D57DF0>, root_client=<openai.OpenAI object at 0x0000016025DD8040>, root_async_client=<openai.AsyncOpenAI object at 0x0000016025DD96F0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. What are some methods mentioned for collecting feedback to improve applications?\\n\\nYou can collect feedback by using annotation queues and logging user feedback. Additionally, setting up new feedback criteria and annotating traces inline in the UI are mentioned methods.\\n\\n2. How can you manage datasets in the context provided?\\n\\nDatasets can be managed by creating them with the SDK or from the UI, running offline and online evaluations, analyzing evaluation results in the UI, and logging user and expert feedback.\\n\\n3. What are some evaluation strategies mentioned in the context?\\n\\nThe context mentions online evaluation, offline evaluation, automatic evaluation, setting up LLM-as-judge and custom code evaluators, creating a few-shot evaluator, and auditing and correcting evaluator scores.\\n\\n4. What are some features related to analyzing experiments?\\n\\nYou can analyze a single experiment, compare experiments with the comparison view, filter experiments, view pairwise experiments, fetch experiment results in the SDK, upload experiments run outside of LangSmith with the REST API, download experiment results as a CSV, and rename an experiment.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the response from the LLM\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"Evaluate your application over production traffic\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Evaluate your application over production traffic',\n",
       " 'context': [Document(id='70b17ada-411c-4ad8-b820-79a82d901a21', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Annotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.\\n\\nUse annotation queues\\nLog user feedback\\nSet up a new feedback criteria\\nAnnotate traces inline in the UI\\nAudit and correct evaluator scores\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousTest a ReAct agent with Pytest/Vitest and LangSmithNextAnalyze a single experimentKey featuresOffline evaluationRun an evaluationDefine an evaluatorConfigure the evaluation dataConfigure an evaluation jobTesting integrationsOnline evaluationAutomatic evaluationAnalyzing experiment resultsDataset managementAnnotation queues and human feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(id='df1eb56e-d9c3-4336-a94e-efd5b77dcc83', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationHow-to GuidesOn this pageEvaluation how-to guides'),\n",
       "  Document(id='6120bbb9-9215-475d-99a3-ee454cba2860', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content=\"Online evaluation\\u200b\\nEvaluate and monitor your system's live performance on production data.\\n\\nSet up an LLM-as-judge online evaluator\\nSet up a custom code online evaluator\\nCreate a few-shot evaluator\\n\\nAutomatic evaluation\\u200b\\nSet up evaluators that automatically run for all experiments against a dataset.\\n\\nSet up an auto-evaluator\\nCreate a few-shot evaluator\\n\\nAnalyzing experiment results\\u200b\\nUse the UI & API to understand your experiment results.\\n\\nAnalyze a single experiment\\nCompare experiments with the comparison view\\nFilter experiments\\nView pairwise experiments\\nFetch experiment results in the SDK\\nUpload experiments run outside of LangSmith with the REST API\\nDownload experiment results as a CSV\\nAudit and correct evaluator scores\\nRenaming an experiment\\n\\nDataset management\\u200b\\nManage datasets in LangSmith used by your evaluations.\"),\n",
       "  Document(id='94c20e16-81fc-4723-aa8d-d9d62226714e', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Create a dataset with the SDK or from the UI\\nRun offline evaluations with the SDK or from the UI\\nRun online evaluations with LLM-as-judge and custom code evaluators\\nAnalyze evaluation results in the UI\\nLog user feedback from your app\\nLog expert feedback with annotation queues\\n\\nOffline evaluation\\u200b\\nEvaluate and improve your application before deploying it.\\nRun an evaluation\\u200b\\n\\nDefine a target function to evaluate\\nRun an evaluation with the SDK\\nRun an evaluation asynchronously\\nRun an evaluation comparing two experiments\\nEvaluate a langchain runnable\\nEvaluate a langgraph graph\\nEvaluate an existing experiment (Python only)\\nRun an evaluation from the UI\\nRun an evaluation via the REST API\\nRun an evaluation with large file inputs\\n\\nDefine an evaluator\\u200b')],\n",
       " 'answer': '1. What are some methods mentioned for collecting feedback to improve applications?\\n\\nYou can collect feedback by using annotation queues and logging user feedback. Additionally, setting up new feedback criteria and annotating traces inline in the UI are mentioned methods.\\n\\n2. How can you manage datasets in the context provided?\\n\\nDatasets can be managed by creating them with the SDK or from the UI, running offline and online evaluations, analyzing evaluation results in the UI, and logging user and expert feedback.\\n\\n3. What are some evaluation strategies mentioned in the context?\\n\\nThe context mentions online evaluation, offline evaluation, automatic evaluation, setting up LLM-as-judge and custom code evaluators, creating a few-shot evaluator, and auditing and correcting evaluator scores.\\n\\n4. What are some features related to analyzing experiments?\\n\\nYou can analyze a single experiment, compare experiments with the comparison view, filter experiments, view pairwise experiments, fetch experiment results in the SDK, upload experiments run outside of LangSmith with the REST API, download experiment results as a CSV, and rename an experiment.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='70b17ada-411c-4ad8-b820-79a82d901a21', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Annotation queues and human feedback\\u200b\\nCollect feedback from subject matter experts and users to improve your applications.\\n\\nUse annotation queues\\nLog user feedback\\nSet up a new feedback criteria\\nAnnotate traces inline in the UI\\nAudit and correct evaluator scores\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousTest a ReAct agent with Pytest/Vitest and LangSmithNextAnalyze a single experimentKey featuresOffline evaluationRun an evaluationDefine an evaluatorConfigure the evaluation dataConfigure an evaluation jobTesting integrationsOnline evaluationAutomatic evaluationAnalyzing experiment resultsDataset managementAnnotation queues and human feedbackCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(id='df1eb56e-d9c3-4336-a94e-efd5b77dcc83', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationHow-to GuidesOn this pageEvaluation how-to guides'),\n",
       " Document(id='6120bbb9-9215-475d-99a3-ee454cba2860', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content=\"Online evaluation\\u200b\\nEvaluate and monitor your system's live performance on production data.\\n\\nSet up an LLM-as-judge online evaluator\\nSet up a custom code online evaluator\\nCreate a few-shot evaluator\\n\\nAutomatic evaluation\\u200b\\nSet up evaluators that automatically run for all experiments against a dataset.\\n\\nSet up an auto-evaluator\\nCreate a few-shot evaluator\\n\\nAnalyzing experiment results\\u200b\\nUse the UI & API to understand your experiment results.\\n\\nAnalyze a single experiment\\nCompare experiments with the comparison view\\nFilter experiments\\nView pairwise experiments\\nFetch experiment results in the SDK\\nUpload experiments run outside of LangSmith with the REST API\\nDownload experiment results as a CSV\\nAudit and correct evaluator scores\\nRenaming an experiment\\n\\nDataset management\\u200b\\nManage datasets in LangSmith used by your evaluations.\"),\n",
       " Document(id='94c20e16-81fc-4723-aa8d-d9d62226714e', metadata={'source': 'https://docs.smith.langchain.com/evaluation/how_to_guides', 'title': 'Evaluation how-to guides | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions.', 'language': 'en'}, page_content='Create a dataset with the SDK or from the UI\\nRun offline evaluations with the SDK or from the UI\\nRun online evaluations with LLM-as-judge and custom code evaluators\\nAnalyze evaluation results in the UI\\nLog user feedback from your app\\nLog expert feedback with annotation queues\\n\\nOffline evaluation\\u200b\\nEvaluate and improve your application before deploying it.\\nRun an evaluation\\u200b\\n\\nDefine a target function to evaluate\\nRun an evaluation with the SDK\\nRun an evaluation asynchronously\\nRun an evaluation comparing two experiments\\nEvaluate a langchain runnable\\nEvaluate a langgraph graph\\nEvaluate an existing experiment (Python only)\\nRun an evaluation from the UI\\nRun an evaluation via the REST API\\nRun an evaluation with large file inputs\\n\\nDefine an evaluator\\u200b')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
